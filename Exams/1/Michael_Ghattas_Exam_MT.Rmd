---
title: "COMP 4442 Midterm, Winter 2024"
author: "Michael_Ghattas_Exam_MT"
output: pdf_document
---

```{r setup, include=TRUE}
knitr::opts_chunk$set(echo = TRUE)

# Load required libraries
library(leaps)        # For regression and model selection
library(ggplot2)      # For visualization
library(ggpubr)       # For Q-Q plots
library(dplyr)        # For data manipulation
library(car)          # For hypothesis testing

# Load datasets for the exam
q1_data <- read.csv("Q1data.csv")
q2_data <- read.csv("Q2data.csv")
q3_data <- read.csv("Q3data.csv")
q4_data <- read.csv("Q4data.csv")
q5_data <- read.csv("Q5data.csv")
q6_data <- read.csv("Q6data.csv")

# Display first few rows of each dataset for initial inspection
head(q1_data)
head(q2_data)
head(q3_data)
head(q4_data)
head(q5_data)
head(q6_data)
```

There are six questions on this midterm, all of which have multiple parts. Please be sure to provide answers to all parts of each question. Each question has an associated .csv file, which you will load into memory at the beginning of the question. All of the included data sets are simulated, so any results should not be taken as evidence for or against the existence of anything in the real world. The data were simulated to minimize the ambiguity and messiness that typifies real data. If you feel that something is ambiguous in a way that impedes your ability to answer the questions, please let me know. I believe in you! 

## Question 1: Basic ANOVA - 10 points total

A tire manufacturing company wants to know if different formulations of tire rubber result in differences in tire durability. They are interested in four different rubber formulations ("form"). To test this, 20 tires of each rubber formulation are selected for testing. Aside from the rubber formulation, all 80 tires in this experiment are otherwise exactly the same. The durability of each tire is tested using a durability machine, which mimics the forces and stress a tire is exposed to when installed in a standard sedan driving down a flat asphalt road at 60 miles per hour. The machines tracks how many miles the tire has "traveled" based on the number of rotations of the tire. The durability test stops when the tire's structure fails, which is when the durability machine records the number of traveled miles ("miles"). The data from this hypothetical experiment is contained in the Q1data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question. 
```{r }
# Load the data
tires <- read.csv("Q1data.csv", header = TRUE, sep = ",")

# Convert 'form' to a factor
tires$form <- as.factor(tires$form)

# Display structure of the data
str(tires)
```

# Q1, Part 1: Assessing the normality of groups assumption (2 points)

You will assess the assumption of normality in two ways: quantitatively and visually.
In this first code chunk, please conduct an appropriate *quantitative* assessment of the normality assumption and display the results.
```{r }
# Quantitative assessment of normality using the Shapiro-Wilk test
shapiro_results <- by(tires$miles, tires$form, shapiro.test)

# Display the results
shapiro_results
```

In this second code chunk, please conduct an appropriate *visual* assessment of the normality assumption and display the visualization/s you create. 
```{r }
# Visual assessment of normality using Q-Q plots for each formulation
ggqqplot(tires, x = "miles", facet.by = "form", title = "Q-Q Plot for Tire Durability by Rubber Formulation")
```

# Q1, Part 2: Assessing the equality of variances of groups assumption (2 points)

You will assess the assumption of equality of variances in two ways: quantitatively and visually.
In this first code chunk, please conduct an appropriate *quantitative* assessment of the equality of variances assumption and display the results.
```{r }
# Quantitative assessment of equality of variances using Levene's Test
levene_test_result <- leveneTest(miles ~ form, data = tires)

# Display the result of Levene's Test
levene_test_result
```

In this second code chunk, please conduct an appropriate *visual* assessment of the equality of variances assumption and display the visualization/s you create. 
```{r }
# Visual assessment of equality of variances using a boxplot
ggplot(tires, aes(x = form, y = miles, fill = form)) +
  geom_boxplot() +
  labs(title = "Boxplot of Miles by Rubber Formulation", x = "Form", y = "Miles") +
  theme_minimal()
```

# Q1, Part 3: Fitting the ANOVA model (2 points)

Now, you will conduct an ANOVA on the tires data set that can provide an answer to the research question: do different formulations of tire rubber have different durability? Please be sure to display the results of your analysis. 
```{r }
# Fit the ANOVA model
tires.aov <- aov(miles ~ form, data = tires)

# Display the results of the ANOVA
summary(tires.aov)
```

# Q1, Part 4: Interpreting the ANOVA results (4 points)

A) What is the null hypothesis being tested by the ANOVA you conducted? Be specific.

Your answer here: The null hypothesis (H0) for the ANOVA is that there is no difference in mean tire durability (measured in miles) across the four different rubber formulations. In other words, the mean tire durability for all formulations is the same.

B) What is the alternative hypothesis being tested by the ANOVA you conducted? Be specific.

Your answer here: The alternative hypothesis (H1) is that at least one rubber formulation has a different mean tire durability compared to the others. In other words, not all means are equal.

C) Based on the results of your analysis, do you *reject* or *fail to reject* this null hypothesis?

Your answer here: Based on the ANOVA results, the p-value is 0.793, which is greater than the standard significance level of 0.05. Therefore, we fail to reject the null hypothesis. This means that there is no statistically significant evidence to suggest that the mean tire durability differs across the four rubber formulations.

D) Which of the following statements is best supported by the result of this analysis?
Statement 1: There is evidence to suggest that at least one tire formulation has different mean durability than the other formulation
Statement 2: There is evidence to suggest that all tire formulations have different mean durability
Statement 3: There is no evidence to suggest that there are differences in mean durability across tire formulations

Your answer here: There is no evidence to suggest that there are differences in mean durability across tire formulations, based on the high p-value (0.793) obtained from the ANOVA. (Statement 3)

## Question 2: Multifactor ANOVA - 10 points 

A health researcher designed an experiment to test the effects of two medications, Lowesterol and Lipidown, on LDL cholesterol levels of people who had been diagnosed as having high cholesterol but no other health problems. He recruited 160 participants, all of whom took two pills each day for 90 days. For 40 participants, both pills were placebos. For 40 participants, one pill contained Lowesterol and the other pill was a placebo. For 40 participants, one pill contained Lipidown and the other pill was a placebo. For the last 40 participants, one pill contained Lowesterol and the other contained Lipidown. After 90 days, each participant gave a blood sample and the LDL level in their blood was recorded. The data from this hypothetical experiment is contained in the Q2data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }
# Load the data
drugs <- read.csv("Q2data.csv", header = TRUE, sep = ",")

# Convert 'lowesterol' and 'lipidown' to factors
drugs$lowesterol <- as.factor(drugs$lowesterol)
drugs$lipidown <- as.factor(drugs$lipidown)

# Display the structure of the dataset
str(drugs)
```

# Q2, Part 1: Fitting the factorial ANOVA model (4 points)

Now, you will conduct a two-way ANOVA with an interaction on the drug data. Use post.ldl as the outcome. Please be sure to display the results of your analysis. 
```{r }
# Fit the two-way ANOVA model with interaction
drugs.aov <- aov(post.ldl ~ lowesterol * lipidown, data = drugs)

# Display the results of the ANOVA
summary(drugs.aov)
```

# Q2, Part 2: Interpreting the factorial ANOVA model (6 points)

Use the output from the factorial ANOVA to answer the following three questions.

A) Is the main effect of Lowesterol significant?

Your answer here (yes/no): No. 

B) Is the main effect of Lipidown significant?

Your answer here (yes/no): Yes.

C) Is the interaction between Lipidown and Lowesterol significant?

Your answer here (yes/no): No.

## Question 3: Multiple Regression - 20 points total

A security firm contracted by a shopping district wants to examine the factors that contribute to "loss" (theft of money or goods by customers or employees of a store) in the 200 stores in the shopping district. They have four pieces of information about each store: the amount of loss in dollars ("loss", continuous), the area of the store in square feet ("area", continuous), the average number of people who walk into the store on a weekly basis ("traffic", continuous), and whether the store is primarily a retail-oriented store (retail=1) or a service-oriented store (retail=0). The data from this hypothetical study is contained in the Q3data.csv file.

Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }
# Load the data
mall <- read.csv("Q3data.csv", header = TRUE, sep = ",")

# Convert 'retail' to a factor
mall$retail <- as.factor(mall$retail)

# Display the structure of the dataset
str(mall)
```

# Q3, Part 1: Fitting the regression model (2 points)

Now, you will conduct a multiple regression analysis and display the results. Do not include interaction terms or polynomial terms, and do not apply any transformations. 
Outcome: loss.
Predictors: retail, area, and traffic. 
```{r }
# Fit the multiple regression model
mall.reg <- lm(loss ~ retail + area + traffic, data = mall)

# Display the results of the regression
summary(mall.reg)
```

# Q3, Part 2: Checking diagnostic plots (4 points)

Please display the diagnostic plots for the model you fit in the previous part of this question and answer the question below: 
```{r }
# Plot diagnostic plots for the regression model
plot(mall.reg)
```

A) What is the most obvious problem that all of the diagnostic plots for this model share? Be specific. 

Your answer here: The most obvious problem across all the diagnostic plots is the presence of outliers and influential points. Specifically, data points 200, 58, and 55 show up prominently across several diagnostic plots (Residuals vs Fitted, Scale-Location, Residuals vs Leverage), indicating that they are potentially influential observations. Additionally, the Scale-Location plot suggests heteroscedasticity (non-constant variance), as the spread of residuals increases with fitted values, which violates one of the key assumptions of linear regression.

B) What would be a good solution to this specific problem? 

Your answer here: A good solution to the problem of influential points is to investigate and potentially remove, transform these outliers, and consider whether they should be removed or treated differently.

# Q3, Part 3: Re-fitting the regression model (4 points)

Now, implement the solution you proposed in the last part and re-fit the regression model. Be sure to display the results of your updated analysis.
```{r }
# Identify and remove the influential outliers (points 200, 58, 182)
outliers <- c(200, 58, 182)
mall_clean <- mall[-outliers, ]

# Re-fit the regression model with the log-transformed dependent variable (after removing outliers)
mall.reg.change <- lm(loss ~ retail + area + traffic, data = mall_clean)

# Display the results of the updated regression model
summary(mall.reg.change)
```

Next, display the updated diagnostic plots and answer the question below
```{r }
# Display the updated diagnostic plots for the re-fitted model
plot(mall.reg.change)
```

C) Did your solution to the problem you identified in Q3, Part 2 greatly improve the diagnostic plots of the model? (Hint: If your answer isn't 'yes', try a different solution.)

Your answer here: Yes, removing the influential outliers (points 200, 58, and 182) greatly improved the diagnostic plots. The spread of residuals now appears much more evenly distributed, with no clear patterns or large deviations from linearity. This indicates that the model now better meets the assumption of homoscedasticity. The residuals in the Q-Q plot follow the theoretical normal distribution line more closely, indicating that the normality assumption is better met after removing the outliers. The spread of residuals is now much more consistent, indicating that the heteroscedasticity issue has been largely addressed. No highly influential points are evident, and the Cook's distance is no longer showing any extreme values, indicating that the model is not being unduly influenced by any single observation. Thus, the solution has significantly improved the model's fit and its adherence to the assumptions of linear regression.

# Q3, Part 4: Interpreting the re-fitted regression model (10 points)

D) Select the statement that is a correct interpretation of the intercept:
Statement 1: The predicted loss for a retail-oriented store with an area of zero square feet and average weekly traffic of zero people is 3.60 USD. 
Statement 2: The predicted loss for any store is 3.60 USD.
Statement 3: The predicted loss for a store with zero revenue from retail sales, an area of zero square feet, and average weekly traffic of zero people is 3.60 USD. 
Statement 4: The predicted loss for a service-oriented store with an area of zero square feet and average weekly traffic of zero people is 3.60 USD. 
Statement 5: The predicted loss for a store with the mean revenue from retail sales, the mean area, and the mean average weekly traffic is 3.60 USD.

Your answer here: The predicted loss for a service-oriented store with an area of zero square feet and average weekly traffic of zero people is 3.60 USD. Statement 4, because the intercept represents the predicted loss for a service-oriented store (retail = 0), with zero area and zero traffic. Meaning, the intercept is 3.539, meaning that when all predictor variables (retail, area, and traffic) are equal to zero, the predicted loss is 3.54.

E) Select the statement that is a correct interpretation of the coefficient associated with retail:
Statement 1: Holding area and average weekly traffic constant, the predicted loss for a service-oriented store is 4.95 USD higher than for a retail-oriented store. 
Statement 2: The predicted loss for a retail-oriented store is 4.95 USD.
Statement 3: Holding area and average weekly traffic constant, the predicted loss for a retail-oriented store is 4.95 USD higher than for a service-oriented store. 
Statement 4: Holding area and average weekly traffic constant, the predicted loss for a retail-oriented store increases by 4.95 USD for each additional dollar of revenue from retail sales. 
Statement 5: The predicted loss for a retail-oriented store is 8.55 USD.

Your answer here: Holding area and average weekly traffic constant, the predicted loss for a retail-oriented store is 4.95 USD higher than for a service-oriented store. Statement 3, because this correctly interprets the coefficient as the difference in predicted loss between retail and service-oriented stores, holding other variables constant. i.e. The coefficient for retail1 is 4.944, meaning that holding area and traffic constant, the predicted loss for a retail-oriented store (retail = 1) is 4.95 USD higher than for a service-oriented store (retail = 0).

F) What is the predicted amount of loss for a non-retail store that has an area of 1000 square feet and average weekly traffic of 200? Show your work in the code chunk and type your answer below. 
```{r}
# Predicted loss for a non-retail store (retail = 0) with area = 1000 and traffic = 200
intercept <- 3.5395202
beta_retail <- 4.9437873
beta_area <- 0.9996377
beta_traffic <- 0.5005326

retail_value <- 0  # Non-retail store
area_value <- 1000  # 1000 square feet
traffic_value <- 200  # 200 average weekly traffic

# Calculate predicted loss
predicted_loss <- intercept + (beta_retail * retail_value) + (beta_area * area_value) + (beta_traffic * traffic_value)
predicted_loss
```

Your answer here: The predicted amount of loss for a non-retail store that has an area of 1000 square feet and average weekly traffic of 200 is $1103.28.

---

## Question 4: Automated model selection - 30 points total

The data set Q4data.csv contains nine variables: y, x1, x2, x3, x4, x5, x6, x7, and x8. All of these variables are continuous. 
Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }
# Load the dataset
many.var <- read.csv("Q4data.csv", header = TRUE, sep = ",")

# Display the structure of the dataset
str(many.var)
```

# Q4, Part 1: Forward selection - 10 points

First, you will use forward selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Be sure to include trace=1 as part of your use of the function. After this, display the model selected using forward selection. 
```{r }
# Start with an empty model
null_model <- lm(y ~ 1, data = many.var)

# Define the full model with all predictors
full_model <- lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = many.var)

# Perform forward selection
forward.model <- step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward", trace = 1)
```

The model selected by forward selection:
```{r }
# Display the model selected using forward selection
summary(forward.model)
plot(forward.model)
```

# Q4, Part 2: Backward selection - 10 points

Next, you will use backward selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Be sure to include trace=1 or trace=TRUE as part of your use of the function. After this, display the model selected using backward selection. 
```{r }
# Start with the full model containing all predictors
full_model <- lm(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = many.var)

# Perform backward selection
backward.model <- step(full_model, direction = "backward", trace = 1)
```

The model selected by backward selection:
```{r }
# Display the selected model from backward selection
summary(backward.model)
plot(backward.model)
```

# Q4, Part 3: Best subsets selection - 10 points

Finally, you will use best subsets selection to select a model. The outcome will be y and the pool of potential predictors will include x1, x2, x3, x4, x5, x6, x7, and x8. Recall from the best subsets demo that you need to include all of the following for full credit:
1) Correctly set up the inputs into the regsubsets() function
2) Correctly run the regsubsets() function
3) Display either the star table or T/F table. 
4) Display either a list of model BICs or a scatterplot of the model BICs. 
5) Display the output of the model selected using best subsets selection. 
```{r }
# Set up inputs into regsubsets and fitting models using all the predictors x1 to x8
best_subset_fit <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = many.var, nvmax = 8)
```

```{r }
# Run the regsubsets function to evaluate all possible models
best_subset_fit <- regsubsets(y ~ x1 + x2 + x3 + x4 + x5 + x6 + x7 + x8, data = many.var, nvmax = 8)
```

```{r }
# Display the summary of the regsubsets output i.e. True/False table for which predictors are included in each model
best_subset_summary <- summary(best_subset_fit)

# Display the T/F table i.e. the star table
best_subset_summary$outmat
```

```{r }
# Display the BIC values for each model
best_subset_summary$bic
# Create a scatterplot of BICs
plot(best_subset_summary$bic, type = "b", main = "BIC for Best Subsets", xlab = "Number of Predictors", ylab = "BIC")
```

```{r }
# Identify the best model and extract the coefficients of the best model selected by BIC
best_bic_model <- which.min(best_subset_summary$bic)

# Display the coefficients of the selected model
coef(best_subset_fit, best_bic_model)
```

These are the selected variables based on minimizing the BIC. The BIC plot clearly shows that the model with 5 predictors minimizes the BIC value, indicating the best model according to Bayesian Information Criterion. These are the best subsets selection identified the following model with the predictors. Thus, this model offers the best balance between model fit and complexity, as judged by the BIC.

## Question 5: Nested model selection - 20 points total 

The data set Q5data.csv contains nine variables: y, x1, x2, x3, x4, x5, x6, x7, and x8. All of these variables are continuous. This is the same data set used in Question 4, but please reload the data set under a new name to ensure no "cross-contamination" between questions. 
Run the code chunk below to load the data into memory before beginning your work on this question.
```{r }
# Load the dataset
Q5.var <- read.csv("Q5data.csv", header = TRUE, sep = ",")

# Display the structure of the dataset
str(Q5.var)
```

# Q5, Part 1: Identifying nested models - 10 points

I fitted five regression models using different sets of predictors. Run the code chunk below to estimate and view the models I fitted. Review the output for these models and answer the questions below.
```{r }
# Fit the models
model.1 = lm(y ~ x1, data = Q5.var)
model.2 = lm(y ~ x1 + x2, data = Q5.var)
model.3 = lm(y ~ x1 + x3, data = Q5.var)
model.4 = lm(y ~ x1 + x2 + x3, data = Q5.var)
model.5 = lm(y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3, data = Q5.var)

# Display the summaries of the models
summary(model.1)
summary(model.2)
summary(model.3)
summary(model.4)
summary(model.5)
```

A) If Model 5 (model.5) is considered to be the "full model", which of the remaining models - Models 1, 2, 3, and 4 - are nested relative to it?

Your answer here: Models 1, 2, 3, and 4 are all nested within Model 5. A model is nested if it can be derived by removing one or more terms (predictors or interactions) from the full model. Since Model 5 includes all the terms and interaction terms involving x1, x2, and x3, it is the full model. Each of the other models is a simplified version, lacking some terms or interactions that are included in Model 5.

B) If Model 4 (model.4) is considered to be the "full model", which of the remaining models - Models 1, 2, and 3 - are nested relative to it? 

Your answer here: Models 1, 2, and 3 are all nested within Model 4. Model 4 includes the main effects of x1, x2, and x3, while Models 1, 2, and 3 are reduced models containing only subsets of these predictors. For instance, Model 1 includes only x1, Model 2 includes x1 and x2, and Model 3 includes x1 and x3.

C) If Model 3 (model.3) is considered to be the "full model", which of the remaining models - Models 1 and 2 - are nested relative to it? 

Your answer here: Model 1 is nested within Model 3. Model 1 includes only x1, while Model 3 includes both x1 and x3. Model 2 is not nested within Model 3 because it includes x2, which is absent from Model 3.

D) In the code chunk below, specify a new model that is nested relative to Model 5 AND in which Model 2 is nested. That is, specify a model that fits the nested model relationship depicted below:
Model 5 (7 predictor coefficients) <-- (Your model, 3-6 predictor coefficients) <-- Model 2 (2 predictor coefficients)
Please note that you cannot chose any of the models already fitted in this question. You must specify a model that hasn't yet been fitted. 
```{r }
# A new model that is nested within Model 5 and contains Model 2 (x1 and x2)
model.new <- lm(y ~ x1 + x2 + x1:x2:x3, data = Q5.var)

# Display the summary of the new model
summary(model.new)
```

# Q5, Part 2: Nested model testing - 10 points

For this part, you will conduct two nested model tests. In the first test, you will test Model 2 and the new model you specified. In the second test, you will test the new model you specified and Model 5. After you've done this, answer the two questions below. 
```{r }
# Perform the nested model test between Model 2 and model.new
m2.vs.new <- anova(model.2, model.new)

# Display the result of the nested model comparison
m2.vs.new
```

```{r }
# Perform the nested model test between model.new and Model 5
new.vs.m5 <- anova(model.new, model.5)

# Display the result of the nested model comparison
new.vs.m5
```

E) Based on the result of the test between Model 2 and your new model, which model would you choose?

Your answer here: The test between Model 2 (which includes x1 and x2) and the new model (which includes x1, x2, and the interaction term x1:x2:x3) resulted in a significant p-value (p = 0.01659), indicating that the inclusion of the interaction term x1:x2:x3 significantly improves the model's fit. Since the interaction term provides a significant improvement, I would choose the new model over Model 2.

F) Based on the result of the test between your new model and Model 5, which model would you choose? 

Your answer here: The test between the new model (which includes x1, x2, and the interaction term x1:x2:x3) and Model 5 (which includes all the main effects and interaction terms involving x1, x2, and x3) resulted in a p-value of 0.04974. This indicates that Model 5 provides a statistically significant improvement over the new model. Therefore, I would choose Model 5 because it significantly improves the fit compared to the new model.

## Question 6: Basic logistic regression - 10 points total

A state public health agency wants to investigate the presence of dangerous amounts of lead in drinking water across households within the state. Investigators collected tap water samples from 150 single-family homes and obtained information about each house. Based on advice from an environmental agency, the investigators classified a tap water sample as being safe if it had levels below 15 parts per billion (danger=0) or potentially dangerous if it had levels equal to or greater than 15 parts per billion (danger=1). In addition, they tested the "hardness" (i.e, presence of dissolved calcium, magnesium, and other minerals) of the water sample, which they categorized as being low (hard=0) or high (hard=1). They also noted the age of the house in years and the location type of the house (urban, suburban, or rural). The data from this hypothetical study is contained in the Q6data.csv file. 

Run the code chunk below to load the data into memory before beginning your work on this question
```{r }
# Load the dataset
lead <- read.csv("Q6data.csv", header = TRUE, sep = ",")

# Display the structure of the dataset
str(lead)
```

# Q6, Part 1: Fitting a logistic model - 5 points

Fit a logistic regression model using "danger" (categorical) as the outcome and "age" (continuous), "loc" (categorical), and "hard" (categorical) as predictors. Do not apply any transformations or include interaction terms or polynomial terms. Be sure to display the results of the analysis.
```{r }
# Convert 'loc' and 'hard' to factors
lead$loc <- as.factor(lead$loc)
lead$hard <- as.factor(lead$hard)

# Fit a logistic regression model with the binary outcome "danger" and predictors age, loc, and hard
danger.model <- glm(danger ~ age + loc + hard, data = lead, family = binomial)

# Display the summary of the logistic regression model
summary(danger.model)
```

# Q6, Part 2: Interpreting a logistic model - 5 points

A) Based on the results of your analyses, which predictor coefficients (i.e, not including the intercept) were significantly different from zero? There is at least one. 

Your answer here: The predictor age is significantly different from zero, with a p-value of 0.0044. The other predictors (locsuburban, locurban, and hard1) are not significantly different from zero.

B) Of the predictor/s you listed in sub-question A (the sub-question right before this one), which predictor/s (if any) indicate that the presence of a dangerous level of lead is *more likely* as the value of the predictor increases? Which predictor/s (if any) indicate that that the presence of a dangerous level of lead is *less likely* as the value of the predictor increases? 

Dangerous level of lead *more likely* as values of predictor/s increase/s (your answer here): The predictor age has a positive coefficient (0.05024), which suggests that as the age of the house increases, the likelihood of having dangerous levels of lead increases.

Dangerous level of lead *less likely* as values of predictor/s increase/s (your answer here): N/A. None of the predictors indicate a decreased likelihood of dangerous lead levels. All significant predictors show an increased likelihood as their values increase. Also, while the negative intercept indicates that the baseline category has a less-than-even chance of dangerous lead levels, it's not significantly different from zero, meaning that we cannot confidently interpret it.

Note: If there are no predictors to list in one of these choices, write N/A.

# End.