---
title: "COMP 4442: Project Report - Multiple Regression Analysis"
author: "Michael Ghattas & Dawnena Key"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
options(repos = c(CRAN = "https://cran.rstudio.com/"))

# Install and force TinyTeX installation if needed
if (!tinytex::is_tinytex()) {
  install.packages("tinytex")
  tinytex::install_tinytex(force = TRUE)
}

# Load required libraries
library(readr)
library(ggplot2)
library(MASS)
library(car)
library(dplyr)
library(glmnet)
library(stats)
library(caret)
library(PerformanceAnalytics)
library(knitr)
library(gridExtra)
library(lmtest)
library(sandwich)
library(broom)
```

# Introduction

The rapid growth of data-related jobs across industries has led to an evolving landscape in job roles, compensation, and skill requirements. This project aims to explore and quantify the factors that influence salaries within the data job market. Using the “Jobs in Data” dataset, which includes variables such as job title, experience level, employment type, company characteristics, and salary information, we seek to understand which factors most significantly impact salary outcomes.

The primary objective of this analysis is to develop predictive models for salary based on key input variables, applying advanced statistical techniques such as Stepwise Regression, Lasso, and Ridge Regression. These models are evaluated to ensure both statistical rigor and practical interpretability, with particular attention to addressing multicollinearity and optimizing model fit. The project also includes exploratory data analysis to visualize initial trends and a final model refinement stage to identify the best predictors.

# Load Dataset

```{r}
# Load the dataset
data <- read_csv("jobs_in_data.csv")

# Display the first few rows
spec(data)
head(data)
```

# Varibles Transformation

```{r}
# Convert selected variables to factors
data <- data %>%
  mutate(
    experience_level = as.factor(experience_level),
    employment_type = as.factor(employment_type),
    work_setting = as.factor(work_setting),
    company_size = as.factor(company_size),
    job_category = as.factor(job_category),
    company_location = as.factor(company_location),
    employee_residence = as.factor(employee_residence)
  )

# Display the first few rows
head(data)
```

# Exploratory Data Analysis (EDA)

### Boxplot: Salary by Experience Level

This boxplot will show the variation in salary for different experience levels (e.g., Entry-level, Mid-level, Senior, Executive). Typically, you expect salary to increase with experience.

```{r}
# Boxplot of Salary by Experience Level
ggplot(data, aes(x = experience_level, y = salary_in_usd)) + 
  geom_boxplot() + 
  theme_minimal() +
  labs(title = "Salary Distribution by Experience Level", x = "Experience Level", y = "Salary (USD)")
```

Interpretation: Salaries tend to increase with experience level, with Executive positions earning the most, indicating that experience is likely an important predictor of salary.

### Boxplot: Salary by Employment Type

This boxplot explores how salary varies by employment type (e.g., Full-time, Part-time, Freelance). Full-time roles typically offer higher salaries.

```{r}
# Boxplot of Salary by Employment Type
ggplot(data, aes(x = employment_type, y = salary_in_usd)) + 
  geom_boxplot() + 
  theme_minimal() +
  labs(title = "Salary Distribution by Employment Type", x = "Employment Type", y = "Salary (USD)")
```

Interpretation: Full-time jobs are generally associated with higher salaries compared to part-time or freelance positions, suggesting that employment type affects salary.

### Scatter Plot: Salary vs Company Size

This scatter plot will show how salary relates to the size of the company. Larger companies might offer higher salaries due to better resources and budgets.

```{r}
# Scatter plot of Salary vs Company Size
ggplot(data, aes(x = company_size, y = salary_in_usd)) +
  geom_jitter(alpha = 0.4) +
  theme_minimal() +
  labs(title = "Salary vs Company Size", x = "Company Size", y = "Salary (USD)")
```
Interpretation: The data suggests that medium sized companies (size M) may offer higher salaries compared to large and small companies.

### Check Outliers

```{r}
# Boxplot of salary to visualize potential outliers
ggplot(data, aes(y = salary_in_usd)) +
  geom_boxplot() +
  theme_minimal() +
  labs(title = "Boxplot of Salary (USD)", y = "Salary (USD)")

# Scatter plot to detect outliers in salary and other features
ggplot(data, aes(x = experience_level, y = salary_in_usd)) +
  geom_point(alpha = 0.4) +
  theme_minimal() +
  labs(title = "Scatter Plot of Salary vs Experience Level", y = "Salary (USD)")
```
Interpretation:

  - The boxplot provides a visual summary of the salary distribution.
  - The majority of salaries are concentrated below 200,000 USD, with a median salary around 100,000 USD.
  - A significant number of outliers are present in the upper salary range, extending well beyond 300,000 USD.
  
```{r}
# Calculate the IQR for salary_in_usd
Q1 <- quantile(data$salary_in_usd, 0.25)
Q3 <- quantile(data$salary_in_usd, 0.75)
IQR <- Q3 - Q1

# Define the outlier bounds
lower_bound <- Q1 - 1.5 * IQR
upper_bound <- Q3 + 1.5 * IQR

# Identify outliers
outliers <- data %>%
  filter(salary_in_usd < lower_bound | salary_in_usd > upper_bound)

# Remove outliers from the dataset
data_cleaned <- data %>%
  filter(salary_in_usd >= lower_bound & salary_in_usd <= upper_bound)

# Check the size of the cleaned dataset
nrow(data_cleaned)
```  

Interpretation:

  - Removing outliers ensures that the model focuses on the majority of the data, reducing the influence of extreme values that might skew the results.
  - The decision to remove these outliers is based on the IQR method, which assumes that values far beyond the 1.5× IQR are extreme.
  - This cleaning process can improve model performance by reducing heteroscedasticity and making assumptions about residual normality more valid.
  - This it might also remove some meaningful variability, especially in salary-related analyses where high salaries could be legitimate and not necessarily errors or noise.

# Modeling Exploration

The initial model assess the relationship between salary and input to identify key predictors of salary.

### Stepwise Regression Model

```{r}
# Fit the initial full model for stepwise selection
initial_model <- lm(salary_in_usd ~ experience_level + employment_type + company_size + job_category,
                    data = data_cleaned)

# Perform stepwise selection
stepwise_model <- stepAIC(initial_model, direction = "both")

# Display the summary and of the stepwise model
summary(stepwise_model)

# Calculate VIFs for multicollinearity assessment
vif(stepwise_model)
```

### Stepwise Regression Model Summary

Model Coefficients

  - Experience Level: Executive - A large positive impact on salary (estimate = 80,548), indicating that executives earn significantly more than the baseline group (likely entry-level). Senior and Mid-level - Both are also associated with positive impacts on salary, with senior-level positions (estimate = 58,713) having a higher effect than mid-level (estimate = 22,270). This aligns with expectations, as higher experience levels typically command higher salaries. All experience levels (Executive, Mid-level, and Senior) are highly significant predictors (p < 0.001).
  - Employment Type: Freelance - Has a significant negative impact on salary (estimate = -46,638), suggesting that freelance roles tend to offer lower pay compared to the baseline employment type. Full-time and Part-time - These are not statistically significant (p > 0.05), meaning they do not show a distinct impact on salary in this model.
 - Company Size: Small (company_sizeS) - A significant negative impact on salary (estimate = -34,972), indicating that smaller companies tend to offer lower salaries, possibly due to fewer resources or smaller budgets. Medium (company_sizeM) - Shows a positive effect on salary (estimate = 8,485) and is statistically significant, suggesting a slight salary premium over small companies.
  - Job Category: Several job categories show significant effects on salary. Positive Impact - Data Science and Research (estimate = 25,398) and Machine Learning and AI (estimate = 38,746) roles have strong positive associations with salary, which aligns with their high demand and specialized skill requirements. Data Architecture and Modeling and Data Engineering also have positive effects on salary, though with smaller magnitudes. Negative Impact - Data Analysis (estimate = -20,468) and Data Quality and Operations (estimate = -30,182) show negative impacts, indicating lower salaries within these categories. Data Management and Strategy also has a negative effect on salary (estimate = -16,723). These results suggest that technical and specialized roles (e.g., Machine Learning, Data Science) tend to offer higher pay than more support-oriented or operational roles (e.g., Data Analysis).

Model Fit

  - R-squared: The model explains about 25.9% of the variability in salary, which is typical for real-world salary models where many unobserved factors can influence pay.
  - Adjusted R-squared: At 0.2572, this is close to the R-squared value, indicating that the model's explanatory power remains stable when adjusting for the number of predictors.

Model Diagnostics

  - Residual Standard Error: 50,270, indicating the average deviation of observed salaries from those predicted by the model.
  - F-statistic: The overall model is highly significant (p-value < 2.2e-16), suggesting that at least some predictors have meaningful associations with salary.

Variance Inflation Factor (VIF) Analysis

  - All VIF values are low, with GVIF values adjusted for degrees of freedom all below 1.1, indicating that multicollinearity is not a concern in this model. This means the predictors do not have excessive linear relationships, which supports the stability and reliability of the model coefficients.

Conclusion

This stepwise regression model identifies key variables associated with salaries in data-related roles. Experience level, job category, and company size have strong associations with salary. Higher experience levels, technical job categories (e.g., Machine Learning, Data Science), and larger company sizes are correlated with higher salaries. The model suggests that freelance roles and employment in smaller companies are associated with lower salaries. Multicollinearity is low, indicating that each predictor contributes unique information to the model without excessive overlap. These findings align with industry trends, where experience, specialized skills, and company size often command higher pay. Further refinement and regularization techniques (like Lasso and Ridge regression) can be explored to handle potential overfitting and improve predictive accuracy.

### Log-Transformed Model

```{r}
# Apply log transformation to salary
data_cleaned$log_salary_in_usd <- log(data_cleaned$salary_in_usd)

# Fit the log-transformed model
log_model <- lm(log_salary_in_usd ~ experience_level + employment_type + company_size + job_category,
                data = data_cleaned)

# Display the summary of the log-transformed model
summary(log_model)

# Calculate VIFs for multicollinearity assessment
vif(log_model)

# Plot diagnostic plots to check assumptions
par(mfrow = c(2, 2))
plot(log_model)
```

### Log-Transformed Model Summary

Model Coefficients

  - Experience Level: Executive - Has the largest positive effect on log salary, with an estimate of 0.69688, meaning executives tend to have significantly higher salaries. Senior and Mid-level - Also have positive effects on salary, with senior-level roles (estimate = 0.56555) showing a larger impact than mid-level (estimate = 0.26496). All experience levels (Executive, Mid-level, and Senior) are highly significant predictors (p < 0.001), suggesting that experience level is a strong determinant of salary.
  - Employment Type: Freelance - Has a significant negative effect on log salary (estimate = -0.50722), indicating lower pay for freelance roles. Full-time - Has a positive effect on salary (estimate = 0.24247), significant at the 0.05 level. Part-time - Has a negative effect, but it is only marginally significant (p = 0.085).
  - Company Size: Medium (company_sizeM) - Positive effect (estimate = 0.10460), indicating slightly higher salaries compared to the baseline (likely large companies). Small (company_sizeS) - Significant negative effect on salary (estimate = -0.32417), suggesting that small companies generally offer lower salaries.
  - Job Category: Positive Impacts - Job categories like Machine Learning and AI (estimate = 0.24355) and Data Science and Research (estimate = 0.16617) show significant positive impacts, aligning with industry demand and competitive compensation. Negative Impacts - Roles in Data Analysis (estimate = -0.17363) and Data Quality and Operations (estimate = -0.30714) have significant negative impacts on salary, reflecting comparatively lower pay in these categories.

Model Fit

  - R-squared: 0.29, meaning the model explains about 29% of the variability in log-transformed salary, which is reasonable given the complexity of salary determinants.
  - Adjusted R-squared: 0.2886, which is close to the R-squared, suggesting that the model doesn’t lose much explanatory power when accounting for the number of predictors.
  - Residual Standard Error: 0.3869, indicating the average deviation of observed log salaries from the predicted values.
  - F-statistic: The overall model is highly significant (p-value < 2.2e-16), meaning that at least some predictors are significantly associated with salary.

Variance Inflation Factor (VIF)

  - All GVIF values (adjusted for degrees of freedom) are below 1.1, indicating low multicollinearity among predictors. This suggests that each predictor contributes unique information to the model, supporting the stability and reliability of coefficient estimates.

Model Diagnostics

  - Residuals vs. Fitted: The plot shows a fairly random scatter of residuals around the zero line, though there may be slight clustering. The absence of a clear pattern indicates that the assumption of homoscedasticity is reasonably met, and the linearity assumption is appropriate for the model. However, there is some slight variation in residual spread, which should be monitored but isn’t necessarily problematic for this model.
  - Normal Q-Q Plot: Most points lie close to the diagonal line, indicating that residuals are approximately normally distributed. However, there is slight deviation at both tails (particularly at the upper end), which suggests minor departures from normality. This is common in real-world data, and unless the deviation is severe, it’s generally acceptable.
  - Scale-Location Plot: The points are spread relatively evenly along the fitted values, with no clear pattern or trend, which supports the assumption of homoscedasticity. The red line is mostly horizontal, indicating that residual variance does not vary systematically with the fitted values. This plot reinforces the Residuals vs. Fitted plot’s indication of acceptable homoscedasticity.
  - Residuals vs. Leverage: A few points, such as those labeled (e.g., observations 8016, 8428, 9149), are close to or slightly beyond the threshold of Cook’s distance (0.5), which suggests they may have some influence on the model. While they do not appear to be highly influential, further inspection of these points is advisable to determine if they represent extreme values or unique cases that could potentially impact model stability. Removing or adjusting for these points could be considered if they are deemed outliers.

Conclusion

The diagnostic plots indicate that the log-transformed model is generally well-fitted to the data with the residuals are approximately normally distributed, with only slight deviations at the tails and both the Residuals vs. Fitted and Scale-Location plots support the assumption of constant variance. There are a few potentially influential observations, but they do not appear to have a substantial impact on the overall model fit. The log-transformation appears to improve the model by addressing skewness in salary and enhancing model assumptions. The model is appropriate for predicting salary, given the satisfactory adherence to linear regression assumptions, though regularization methods (e.g., Lasso or Ridge) could further refine predictor selection and manage any minor multicollinearity or outlier influence.

### Interaction Terms Model

```{r}
# Simply the model and introduce interaction terms
interaction_model_cleaned <- lm(salary_in_usd ~ experience_level * employment_type + experience_level * 
                                  job_category + company_size, data = data_cleaned
)

# Summary and plot of the interaction model (Clean Data)
summary(interaction_model_cleaned)
```

### Interaction Terms Model Summary

Model Coefficients

  - Main Effects: Experience Level - Executive level has a highly significant positive effect on salary (estimate = 98455, p < 0.001), indicating that executives tend to earn significantly higher salaries. However, mid-level and senior experience levels do not have significant main effects on salary in this model, possibly because their effects are moderated by employment type and job category.
  - Company Size: Medium companies show a significant positive effect on salary (estimate = 8284, p < 0.001), while small companies show a significant negative effect (estimate = -32938, p < 0.001), consistent with previous models that suggest smaller companies typically offer lower salaries.
  - Interactions: Experience Level and Employment Type - Some interaction terms between experience level and employment type have coefficients listed as "NA" due to singularities, meaning that these terms are collinear with other variables and were dropped from the model. For instance, interactions involving executive-level employees with freelance, full-time, and part-time employment types are undefined. Experience Level and Job Category -  For Executive Data Analysis interaction is significant (estimate = -63198, p = 0.041), suggesting that executive roles in Data Analysis are associated with lower salaries compared to other executive roles. This may reflect the relatively lower market demand or compensation structure within the Data Analysis category. The remaining interactions involving experience level and job category are largely not significant, indicating that salary variation within job categories is not strongly moderated by experience level for most roles.

Model Fit

  - R-squared and Adjusted R-squared: The multiple R-squared is 0.2639, and the adjusted R-squared is 0.2605, indicating that about 26.05% of the variance in salary is explained by the model. This represents a slight improvement over simpler models but still reflects modest explanatory power. Given the number of predictors, the increase in R-squared is not substantial, suggesting that these interactions, while insightful, do not dramatically enhance predictive accuracy.
  - Residual Standard Error: The residual standard error is 50160, which indicates the average deviation of the observed salaries from the fitted values. This value is similar to previous models, suggesting that adding interactions has not substantially reduced prediction error.

Interpretation of Model Results

  - Interaction Effects: Including interaction terms adds complexity to the model, allowing it to explore whether the impact of experience level on salary depends on employment type and job category. The significant interaction between executive-level experience and the Data Analysis job category suggests that certain combinations of experience level and job category have unique effects on salary.
  - Non-significant Interactions: The majority of interaction terms are not significant, suggesting that the main effects of experience level, employment type, and job category capture much of the relationship with salary without requiring interaction terms. The lack of significance for many interaction terms could also indicate multicollinearity, as seen with the "NA" coefficients.

Model Diagnostics

  - Singularities: Several coefficients are marked as "NA" due to singularities, which occur when predictors are perfectly or nearly perfectly collinear. This is common in models with interaction terms and suggests that some categories (e.g., experience levels within specific employment types) may be redundant.
 - F-statistic: The overall model is highly significant (p < 2.2e-16), meaning that, collectively, the predictors are associated with salary, though not all individual predictors are significant.

Conclusion

This interaction model highlights some conditional relationships in salary data but may not substantially improve predictive power over simpler models. While the interactions provide insights, especially regarding the executive-level experience in certain job categories, the model suffers from multicollinearity issues (as indicated by "NA" coefficients) and still shows modest explanatory power.

# Analytical Modeling

Lasso and Ridge are regularization techniques used to handle multicollinearity and prevent overfitting. We will use these models as part of our analysis.

### Lasso Regression

```{r}
# Define the cleaned response variable after removing outliers
y <- data_cleaned$salary_in_usd

# Prepare the encoded data matrix after cleaning
data_encoded <- model.matrix(salary_in_usd ~ experience_level + employment_type + company_size +
                               job_category, data = data_cleaned)[, -1]

# Lasso regression with cross-validation
lasso_model <- cv.glmnet(data_encoded, y, alpha = 1)
lasso_best_lambda <- lasso_model$lambda.min
lasso_pred <- predict(lasso_model, s = lasso_best_lambda, newx = data_encoded)
lasso_r2 <- 1 - sum((y - lasso_pred)^2) / sum((y - mean(y))^2)

# Display model, coefficients, and R-squared
coef(lasso_model, s = lasso_best_lambda)
lasso_r2
```

### Lasso Regression Model Summary

Model Coefficients

  - Experience Level: Executive - The positive coefficient (80,018) indicates that executive-level positions are associated with a substantial salary increase relative to the baseline (likely entry-level), reflecting the significant impact of experience. Senior and Mid-level - Both levels positively impact salary, with coefficients of 58,278 and 21,796, respectively. This hierarchy in effect sizes aligns with the expectation that higher experience levels correlate with higher salaries.
  - Employment Type: Freelance - The negative coefficient (-45,973) suggests that freelance roles are associated with significantly lower salaries compared to the baseline employment type, likely full-time or hybrid. This result is consistent with common compensation practices where freelance roles may offer less stable income. Full-time - The positive coefficient (19,554) indicates a salary premium for full-time roles, while part-time employment has a small negative effect (-6,568), suggesting lower compensation for part-time work.
  - Company Size: Medium-sized companies - A positive coefficient (8,564) suggests a modest salary premium, although the effect is not as strong as in larger companies. Small companies - A negative coefficient (-34,826) indicates that small companies generally offer lower salaries compared to the baseline (likely large companies), possibly due to budget constraints or resource limitations.
  - Job Category: Machine Learning and AI - With the highest positive coefficient (37,710), this category aligns with industry demand and high compensation for specialized skills. Data Science and Research - The positive coefficient (24,376) reflects the high value of data science expertise in the job market. Data Quality and Operations and Data Analysis - Both categories have negative coefficients, indicating lower salaries in these fields. This could be due to these roles being more operational and less specialized compared to others like AI and data science. Data Architecture and Modeling and Data Engineering - These roles have positive coefficients (11,762 and 8,968, respectively), indicating competitive salaries within technical roles, though generally less than in Machine Learning and Data Science. Leadership and Management - A small positive coefficient (4,526) suggests a slight salary premium, though this effect is less pronounced than for technical fields like Machine Learning and Data Science.

Model Fit

  - R-squared: The Lasso model explains about 25.9% of the variance in salary (R-squared = 0.2586). While modest, this R-squared value is expected given the many unobserved factors that influence salary. It’s common for salary models to have lower R-squared values due to the influence of personal, company-specific, and external economic factors.

Key Insights

  - Significant Predictors: The model highlights experience level, employment type, company size, and job category as influential predictors. This finding is consistent with expectations, where experience, employment stability, and specialized roles significantly impact salary.
  - Feature Selection: Lasso regularization effectively zeroed out less impactful variables (such as some categories within employment type), enhancing model simplicity and interpretability.
  - Multicollinearity: Work setting was deliberately excluded in this model to allow selection of the best model structure based on other core variables. After selecting the best model, work setting can be reintroduced to see if it further improves the model’s predictive accuracy or interpretation.

Conclusion

The Lasso model suggests that salary is strongly influenced by experience level, specialized technical roles, and employment type. Executive experience, technical roles like Machine Learning and Data Science, and full-time employment show substantial positive impacts on salary, while freelance and operational roles are associated with lower pay. Lasso regularization provides a simplified model by omitting variables with minimal predictive power, thereby focusing on significant factors and reducing potential overfitting.

### Ridge Regression

```{r}
# Ridge regression with cross-validation
ridge_model <- cv.glmnet(data_encoded, y, alpha = 0)
ridge_best_lambda <- ridge_model$lambda.min
ridge_pred <- predict(ridge_model, s = ridge_best_lambda, newx = data_encoded)
ridge_r2 <- 1 - sum((y - ridge_pred)^2) / sum((y - mean(y))^2)

# Display Ridge R-squared
# Display model, coefficients and R-squared
coef(ridge_model)
ridge_r2
summary(ridge_model)
```

### Ridge Regression Model Summary

Model Coefficients

  - Experience Level: Executive - The positive coefficient (47,528) suggests a substantial increase in salary for executive roles, though it is somewhat less pronounced than in the Lasso model. Senior - Positive coefficient (32,513), indicating that senior roles command higher salaries compared to the baseline (likely entry-level). Mid-level - The small negative coefficient (-2,392) suggests minimal difference from the baseline group, potentially reflecting reduced impact in the Ridge model compared to the Lasso model.
  - Employment Type: Freelance - The negative coefficient (-40,188) aligns with the trend that freelance roles tend to offer lower salaries compared to the baseline. Full-time - Positive coefficient (21,774) suggests a salary premium for full-time roles, while part-time employment has a negative effect (-20,240), indicating a lower salary for part-time roles.
  Company Size: Medium-sized companies - Positive coefficient (10,524) indicates a modest salary increase compared to the baseline. Small companies - Negative coefficient (-32,237) suggests that small companies are associated with lower salaries, likely due to fewer resources or smaller budgets.
  - Job Category: Machine Learning and AI - With a positive coefficient (24,725), this category shows one of the highest positive effects, aligning with the high demand and compensation for specialized technical skills. Data Science and Research - The positive coefficient (13,413) indicates a salary premium in this category, although less pronounced than in the Lasso model. Data Analysis and Data Quality and Operations - Both categories have significant negative coefficients (-28,195 and -35,104, respectively), indicating lower salaries within these fields. Data Architecture and Modeling and Data Engineering - These roles show positive but smaller coefficients (3,188 and -128, respectively), suggesting a minimal impact on salary relative to the baseline. Leadership and Management - A small negative coefficient (-3,147) suggests that salaries in this category are slightly lower than in the baseline, though the effect is minor compared to technical roles.

Model Fit

  - R-squared: The Ridge model has an R-squared of approximately 25.7%, indicating that it explains about 25.7% of the variance in salary. This is very similar to the Lasso model, as expected since both models target overfitting reduction and improved interpretability.

Key Insights

  - Significant Predictors: Experience level, employment type, company size, and job category remain important predictors. The coefficients for these variables are consistent with expectations: executive roles, full-time employment, and technical fields command higher salaries, while freelance and part-time work tend to be associated with lower pay.
  - Coefficient Shrinkage: Unlike Lasso, Ridge regression does not eliminate any predictors, so all predictors remain in the model with adjusted coefficients that are generally smaller than in the Lasso model. This can make Ridge regression useful when we want to keep all predictors in the model, especially for interpretation purposes.

Conclusion

The Ridge regression model highlights the same general trends as Lasso: salary is significantly influenced by experience level, employment type, and job category. Executive experience, technical roles (like Machine Learning and Data Science), and full-time employment show positive impacts on salary, while freelance and operational roles are associated with lower pay. The Ridge model’s R-squared is close to that of the Lasso model, suggesting comparable explanatory power.

# Model Comparison & Selection

We'll compare the performance of the models using AIC, BIC, R-squared, and Adjusted R-squared.
```{r}
# Model Comparison

# For Stepwise Regression
stepwise_aic <- AIC(stepwise_model)
stepwise_bic <- BIC(stepwise_model)
stepwise_r2 <- summary(stepwise_model)$r.squared
stepwise_adj_r2 <- summary(stepwise_model)$adj.r.squared

# For Lasso Regression (cross-validated)
lasso_n <- length(y)
lasso_log_likelihood <- -0.5 * lasso_n * log(sum((y - lasso_pred)^2) / lasso_n)
lasso_aic <- -2 * lasso_log_likelihood + 2 * length(coef(lasso_model, s = lasso_best_lambda) != 0)
lasso_bic <- -2 * lasso_log_likelihood + log(lasso_n) * length(coef(lasso_model,
                                                                    s = lasso_best_lambda) != 0)
lasso_r2 <- lasso_r2  # Calculated from previous code
lasso_adj_r2 <- 1 - ((1 - lasso_r2) * (lasso_n - 1) / (lasso_n - length(coef(lasso_model,
                                                                    s = lasso_best_lambda))))

# For Ridge Regression (cross-validated)
ridge_log_likelihood <- -0.5 * lasso_n * log(sum((y - ridge_pred)^2) / lasso_n)
ridge_aic <- -2 * ridge_log_likelihood + 2 * length(coef(ridge_model, s = ridge_best_lambda) != 0)
ridge_bic <- -2 * ridge_log_likelihood + log(lasso_n) * length(coef(ridge_model,
                                                                    s = ridge_best_lambda) != 0)
ridge_r2 <- ridge_r2  # Calculated from previous code
ridge_adj_r2 <- 1 - ((1 - ridge_r2) * (lasso_n - 1) / (lasso_n - length(coef(ridge_model,
                                                                    s = ridge_best_lambda))))

# Compile comparison results into a data frame for better readability
comparison_results <- data.frame(
  Model = c("Stepwise Regression", "Lasso Regression", "Ridge Regression"),
  AIC = c(stepwise_aic, lasso_aic, ridge_aic),
  BIC = c(stepwise_bic, lasso_bic, ridge_bic),
  R_squared = c(stepwise_r2, lasso_r2, ridge_r2),
  Adjusted_R_squared = c(stepwise_adj_r2, lasso_adj_r2, ridge_adj_r2)
)

# Display the results for comparison
comparison_results
```

### Model Comparison & Selection Summary

AIC and BIC:

  - Lasso Regression has the lowest AIC (199137.3) and BIC (199265.6) values, indicating the best balance between model fit and complexity among the three models.
  - Ridge Regression follows closely with slightly higher AIC and BIC values than Lasso, suggesting a similar level of fit but with a slightly higher penalty for model complexity.
  - Stepwise Regression has the highest AIC (225239.1) and BIC (225374.5), indicating it’s a more complex model with potentially overfitting, given that it doesn’t use regularization.

R-squared and Adjusted R-squared:

  - Stepwise Regression achieves the highest R-squared (0.2586) and Adjusted R-squared (0.2572), suggesting it explains the most variance in the salary variable.
  - Lasso Regression has a comparable R-squared (0.2586) and Adjusted R-squared (0.2572), indicating it still explains a substantial amount of variance while also simplifying the model by setting some coefficients to zero.
  - Ridge Regression has the lowest R-squared (0.2570) and Adjusted R-squared (0.2556), indicating it explains slightly less variance than Lasso and Stepwise. However, it retains all predictors and shrinks coefficients, addressing multicollinearity without omitting variables.

Conclusion

Lasso Regression appears to be the best model, as it achieves the lowest AIC and BIC while providing a similar R-squared and Adjusted R-squared to Stepwise Regression. Its regularization and feature selection make it a strong choice by improving interpretability and managing multicollinearity.
Stepwise Regression explains the most variance, but its higher AIC and BIC indicate it may be overfitting compared to Lasso and Ridge. Ridge Regression performs similarly to Lasso but keeps all predictors, resulting in slightly lower model fit statistics.

# Final Model

After evaluating Stepwise, Lasso, and Ridge regression models, Lasso regression was chosen as the best model due to its effective feature selection and regularization. Unlike Ridge, which retains all variables, Lasso regression penalizes coefficients by shrinking less important predictors to zero, thus selecting only the most relevant predictors. This helps mitigate overfitting by focusing on the strongest signals in the data and discarding potentially irrelevant features, which is particularly beneficial for datasets with many variables. Additionally, Lasso regression maintains a balance between model simplicity and interpretability, making it the preferred choice for our objective of accurate salary prediction.

### Lasso Regression (Reintroducing work_setting)

After comparing the performance of these models (e.g., based on AIC, BIC, R-squared), we will reintroduce the work_setting variable into the best model.
```{r}
# Define the cleaned response variable after removing outliers
y <- data_cleaned$salary_in_usd

# Prepare the encoded data matrix after cleaning
data_encoded_Final <- model.matrix(salary_in_usd ~ experience_level + employment_type + company_size +
                                     job_category + work_setting, data = data_cleaned)[, -1]

# Lasso regression with cross-validation
lasso_model_Final <- cv.glmnet(data_encoded_Final, y, alpha = 1)
lasso_best_lambda_Final <- lasso_model_Final$lambda.min
lasso_pred_Final <- predict(lasso_model_Final, s = lasso_best_lambda_Final, newx = data_encoded_Final)
lasso_r2_Final <- 1 - sum((y - lasso_pred_Final)^2) / sum((y - mean(y))^2)

# Display model, coefficients, and R-squared
coef(lasso_model_Final, s = lasso_best_lambda_Final)
lasso_r2_Final
```

### Lasso Regression Summary (Reintroducing work_setting)

Model Coefficients

  - Experience Level: Executive - The largest positive coefficient (77,501), indicating that executive-level roles have a substantial impact on salary compared to entry-level roles. Senior and Mid-level - Positive coefficients (54,976 and 19,155, respectively) also reflect increasing salary benefits with experience level. These results confirm that higher experience levels are associated with higher salaries.
  - Employment Type: Freelance - The negative coefficient (-41,030) suggests that freelance roles are associated with lower salaries compared to the baseline employment type (likely full-time or hybrid roles). Full-time - A positive coefficient (17,813) indicates a modest salary increase for full-time roles, while part-time does not appear in the final model, likely indicating that it has minimal predictive impact.
  - Company Size: Medium-sized companies - A positive coefficient (3,614) suggests a small salary increase, although this effect is not as pronounced as in other categories. Small companies - A negative coefficient (-32,626) indicates that small companies are associated with lower salaries, possibly due to fewer resources or smaller budgets.
  - Job Category: Machine Learning and AI - The largest positive impact among job categories (39,510), highlighting high demand and competitive compensation in these technical roles. Data Science and Research - Positive coefficient (26,127), indicating that data science roles are well-compensated, although not to the same extent as machine learning roles. Data Quality and Operations and Data Analysis - Significant negative coefficients (-28,913 and -19,474, respectively) suggest comparatively lower salaries in these support-oriented fields. Data Architecture and Modeling and Data Engineering - Positive coefficients (14,227 and 10,627, respectively), though less substantial than Machine Learning and Data Science, suggest competitive compensation within technical roles. Leadership and Management - A smaller positive coefficient (5,868) reflects a minor salary premium for leadership roles, though the effect is less pronounced than for technical fields.
  - Work Setting: In-person - A positive coefficient (37,640), suggesting that in-person roles may offer a salary premium, potentially due to location-specific demand or additional compensation for physical presence. Remote - A positive coefficient (32,420) indicates a salary premium for remote work, reflecting the flexibility and desirability of remote roles. This aligns with trends showing competitive compensation for remote positions, especially in tech-related fields.
  Reintroducing work_setting to the final Lasso model revealed that this variable significantly influences salary, with remote and in-person roles exhibiting higher salary levels than hybrid work settings. This finding likely reflects current industry trends in data and tech roles, where flexibility in work arrangements, particularly remote work, has become highly valued. Remote roles may command a premium due to demand for flexibility, while in-person roles could reflect increased compensation related to geographic constraints or specialized onsite responsibilities. This reinforces the relevance of work setting as a key predictor in understanding salary variations.

Model Fit

  - R-squared: Approximately 0.2673, meaning the model explains about 26.7% of the variability in salary. This level of explanatory power is typical for salary models, where many unobserved factors influence pay.

Key Insights

  - High-Impact Predictors: Experience level, employment type, job category, and work setting are significant drivers of salary.
  - Regularization Benefits: By selecting relevant predictors and shrinking coefficients of less impactful ones, Lasso improves the model’s interpretability without overfitting, making it ideal for identifying key salary influencers.
  - Work Setting Relevance: The inclusion of work_setting highlights the growing importance of flexibility and location in determining salary, with both in-person and remote roles showing positive impacts.

Conclusion

This final model effectively identifies the key drivers of salary within data-related roles. The significant impact of executive experience, full-time employment, and high-demand job categories (such as Machine Learning and Data Science) aligns with industry expectations. Moreover, the positive coefficients for in-person and remote work settings underscore the value of flexible and location-specific work arrangements. This Lasso model is well-suited for practical applications, balancing explanatory power with simplicity, making it a robust choice for predicting salary outcomes in the data job market.
  
### Lasso Regression Diagnostics (Reintroducing work_setting) 

```{r}
# Calculate residuals from the Lasso model
residuals_lasso <- as.vector(y - lasso_pred_Final)

# Linearity Check: Plot each predictor against the residuals from the Lasso model
cat("\nLinearity Check: Predictor vs Residual Plots\n")

predictors <- colnames(data_encoded_Final)

for (i in seq_along(predictors)) {
  ggplot(data.frame(Predictor = data_encoded_Final[, i], Residuals = residuals_lasso),
         aes(x = Predictor, y = Residuals)) +
    geom_point(alpha = 0.4) +
    geom_smooth(method = "loess", color = "red") +
    labs(title = paste("Residuals vs", predictors[i]),
         x = predictors[i], y = "Residuals") +
    theme_minimal()
}

# Normality of Residuals: KS Test and Histogram
cat("\nNormality of Residuals:\n")

# KS test for normality
ks_test <- ks.test(residuals_lasso, "pnorm", mean(residuals_lasso), sd(residuals_lasso))
print(ks_test)

# Plot histogram of residuals
ggplot(data.frame(residuals = residuals_lasso), aes(x = residuals)) +
  geom_histogram(bins = 30, fill = "lightblue", color = "black") +
  labs(title = "Histogram of Residuals", x = "Residuals", y = "Frequency") +
  theme_minimal()

# Q-Q plot of residuals
ggplot(data.frame(residuals = residuals_lasso), aes(sample = residuals)) +
  stat_qq() +
  stat_qq_line(color = "red") +
  labs(title = "Q-Q Plot of Residuals") +
  theme_minimal()

# Homoscedasticity Check: Breusch-Pagan Test
cat("\nHomoscedasticity Check: Breusch-Pagan Test\n")
bp_test <- bptest(residuals_lasso ~ data_encoded_Final)
print(bp_test)

# Multicollinearity Check: Variance Inflation Factor (VIF)
cat("\nMulticollinearity Check: Variance Inflation Factor\n")
# Re-specify the model for VIF calculation without matrix encoding
vif_model <- lm(salary_in_usd ~ experience_level + employment_type + company_size + job_category +
                  work_setting, data = data_cleaned)
vif_values <- vif(vif_model)
print(vif_values)

# Outlier and Influence Analysis: Cook's Distance
cat("\nInfluence Analysis: Cook's Distance\n")

# Calculate Cook's Distance for each observation using a temporary lm model
temp_model <- lm(y ~ data_encoded_Final)
cooks_distances <- cooks.distance(temp_model)

# Plot Cook's Distance
plot(cooks_distances, type = "h", main = "Cook's Distance", ylab = "Cook's Distance", col = "blue")

# Identify points with high influence based on Cook's Distance > 4/n
n <- length(cooks_distances)
high_influence_points <- which(cooks_distances > (4 / n))

cat("High influence points (Cook's Distance > 4/n):\n")
print(high_influence_points)
```

### Final Model Diagnostics (Reintroducing work_setting)

Cook's Distance (Outlier and Influence Analysis):

  - The Cook's Distance plot shows several data points with high influence, with some exceeding the typical threshold of 4/n, where n is the number of observations. These high-influence points may be affecting the stability of the model. Observations with large Cook's distances indicate data points that could have a disproportionate impact on the model’s fit. Careful consideration should be given to these points, possibly through further analysis or sensitivity testing to assess their influence.

Histogram of Residuals (Normality Check):

  - The histogram of residuals approximates a normal distribution, although there is some skewness and potential outliers on both ends of the distribution. This indicates that the residuals mostly follow a normal distribution, but the presence of outliers or slight asymmetry suggests that the model may not fully capture all complexities in the data. Minor deviations from normality are common in real-world data and may not significantly impact the model, but strong deviations could suggest the need for model adjustments or transformations.

Q-Q Plot (Normality of Residuals):

  - The Q-Q plot shows that the residuals generally follow the 45-degree reference line, indicating that they are approximately normally distributed. Some deviation at the tails is visible, particularly at the extreme values. This suggests the presence of outliers, as observed in the Cook's Distance plot, and indicates that the model may not capture extreme observations accurately. These deviations are typically acceptable unless they significantly impact model interpretation or predictive performance. The Q-Q plot and residuals vs. fitted plot generally confirm model assumptions. However, slight deviations from normality suggest that salary data may retain some skewness. While these minor deviations do not invalidate the model, they indicate that further transformation or more complex models might marginally improve prediction accuracy.

Kolmogorov-Smirnov (KS) Test (Normality Test):

The KS test yielded a p-value of 8.251×$10^{-13}$, which is highly significant. This result indicates that the residuals do not perfectly follow a normal distribution. Although significant, this result is not unusual with large datasets, where even minor deviations can produce a significant p-value. Given the approximate normality observed in the Q-Q plot and histogram, this result may not be critically problematic but should be noted.

Breusch-Pagan Test (Homoscedasticity):
The Breusch-Pagan test for homoscedasticity returned a p-value less than 2.2×$10^{-16}$, suggesting significant heteroscedasticity (non-constant variance) in the residuals. This indicates that the variance of residuals changes across levels of the predictors, potentially affecting the reliability of inference from the model. To address this, we might consider a transformation, robust standard errors, or different modeling approaches (e.g., generalized least squares).

Variance Inflation Factor (VIF Multicollinearity Check):

  - The VIF values for the predictors are all below 1.2, which is well within acceptable limits, indicating low multicollinearity among the predictors. Low multicollinearity suggests that the model’s coefficients are stable and interpretable, with each predictor contributing unique information. This supports the robustness of the model's estimates.

Conclusion

Diagnostic checks were performed to validate model assumptions, including homoscedasticity, normality of residuals, and multicollinearity. The residuals vs. fitted plot largely supports the homoscedasticity assumption, showing a random pattern around zero. However, the Q-Q plot reveals slight deviations from the expected straight line, suggesting a mild skew in the residuals. Although this deviation is minimal, it may indicate that salary data retains some skewness due to the presence of high-income values. While these minor deviations do not compromise the model's overall validity, they suggest that further transformations or alternative modeling approaches, such as log transformation for other predictors, could enhance predictive precision.

### Final Model Tunning

```{r}
# Ensure that data_encoded_Final exists as the final encoded matrix
data_encoded_Final <- model.matrix(salary_in_usd ~ experience_level + employment_type + company_size +
                                     job_category + work_setting, data = data_cleaned)[, -1]

# Convert it to a data frame and add the response variable
data_encoded_df <- as.data.frame(data_encoded_Final)
data_encoded_df$salary_in_usd <- data_cleaned$salary_in_usd

# Fit Robust Linear Model on Original (Untransformed) Salary
robust_model <- lm(salary_in_usd ~ ., data = data_encoded_df)

# Apply robust standard errors to the model
print("Robust Standard Errors for Original Model:")
coeftest(robust_model, vcov = vcovHC(robust_model, type = "HC3"))

# Apply Log Transformation to Salary and Fit Lasso Model
y_log <- log(data_cleaned$salary_in_usd)

# Lasso regression with cross-validation on log-transformed salary
lasso_model_log <- cv.glmnet(data_encoded_Final, y_log, alpha = 1)
lasso_best_lambda_log <- lasso_model_log$lambda.min
lasso_pred_log <- predict(lasso_model_log, s = lasso_best_lambda_log, newx = data_encoded_Final)

# Calculate R-squared for the log-transformed model
lasso_r2_log <- 1 - sum((y_log - lasso_pred_log)^2) / sum((y_log - mean(y_log))^2)

# Display coefficients and R-squared for log-transformed Lasso model
print("Lasso Model Coefficients with Log-Transformed Salary:")
print(coef(lasso_model_log, s = lasso_best_lambda_log))
print(paste("R-squared for Lasso Log Model:", lasso_r2_log))

# Refit robust linear model with original (untransformed) salary and robust SE
robust_model <- lm(salary_in_usd ~ ., data = data_encoded_df)
print("Robust Standard Errors for Final Model:")
coeftest(robust_model, vcov = vcovHC(robust_model, type = "HC3"))
```

```{r}
# Ensure that your robust model is fit as per your provided code
robust_model <- lm(salary_in_usd ~ ., data = data_encoded_df)

# Get robust standard errors and tidy up the results for easier viewing
robust_summary <- coeftest(robust_model, vcov = vcovHC(robust_model, type = "HC3"))
robust_tidy <- tidy(robust_summary)

# Filter to keep only significant predictors (p-value < 0.05)
significant_predictors <- robust_tidy %>%
  filter(p.value < 0.05)

# Display the summary of significant predictors
print("Significant Predictors with Coefficients and P-Values:")
print(significant_predictors)

ggplot(significant_predictors, aes(x = reorder(term, estimate), y = estimate)) +
  geom_bar(stat = "identity", fill = "steelblue") +
  coord_flip() +
  labs(
    title = "Significant Predictors of Salary",
    x = "Predictors",
    y = "Coefficient Estimate"
  ) +
  theme_minimal()
```

### Final Model Tunning Summary

Robust Model with Original Salary

Model Coefficients:

  - Experience Level: Executive, Senior, and Mid-level experience levels are significant predictors of salary, with executives having the highest positive effect (USD 77,951) on salary. These predictors are highly significant, indicating that experience level is a crucial factor in determining salary.
  - Employment Type: Freelance employment has a significant negative impact on salary (USD -41,029), suggesting that freelancers earn considerably less than the baseline employment type. Full-time and Part-time are not statistically significant, implying they do not have a distinct impact on salary in this model.
  - Company Size: Small companies (company_sizeS) have a significant negative effect on salary (USD -32,742), indicating that smaller companies tend to offer lower salaries.
  - Job Category: Several job categories show significant effects:
  - Positive Impacts: Data Architecture and Modeling (USD 15,446), Data Engineering (USD 11,766), Data Science and Research (USD 27,262), and Machine Learning and AI (USD 40,672) are associated with higher salaries.
  - Negative Impacts: Data Analysis (USD -18,393), Data Management and Strategy (USD -16,057), and Data Quality and Operations (USD -28,063) show negative impacts, indicating lower salaries in these roles.
  - Work Setting: Both In-person (USD 39,185) and Remote (USD 33,947) work settings show significant positive effects on salary, indicating that these roles tend to offer higher compensation, possibly due to the flexibility premium.

Lasso Model with Log-Transformed Salary

Model Coefficients:

  - Experience Level: Executive, Senior, and Mid-level have positive coefficients, with Executive level having the highest impact on log-transformed salary (0.668). This result aligns with the robust model, indicating that higher experience levels are associated with higher salaries.
  - Employment Type: Freelance employment has a negative coefficient (-0.448), reinforcing the finding that freelancers tend to earn less. Full-time and Part-time have smaller impacts.
  - Company Size: Small companies have a negative effect (-0.304), suggesting they generally offer lower salaries.
  - Job Category: Similar to the robust model, job categories like Machine Learning and AI (0.251) and Data Science and Research (0.173) are positively associated with higher log-transformed salaries, while Data Analysis (-0.165) and Data Quality and Operations (-0.295) have negative effects.
  - Work Setting: In-person (0.352) and Remote (0.311) work settings have positive impacts on log-transformed salary, suggesting that both settings are associated with higher compensation.
  
Model Diagnostics:

  - The Lasso model with log-transformed salary has an R^2 of approximately 0.302, suggesting that about 30.2% of the variability in log-transformed salary is explained by the model. This transformation helps mitigate the influence of outliers and skewness in salary data. Additionally, the model selects predictors and shrinks less important ones, providing a simpler model and addressing potential multicollinearity. The use of log-transformed salary also reduces the impact of heteroscedasticity.

Conclusion

Both models highlight similar trends; Experience Level and Work Setting consistently have strong positive effects on salary. Freelance Employment Type and Small Company Size are associated with lower salaries. Job Categories reflect significant differences in salary, with roles in Machine Learning, Data Science, and Data Engineering offering higher pay, while operational roles (e.g., Data Quality) offer lower pay. The robust model provides detailed salary estimates with robust standard errors, making it useful for precise interpretation. The Lasso model with log-transformation provides a simplified, regularized view, reducing potential multicollinearity and improving generalizability. Based on these results, the robust model offers interpretability and insights into salary levels, while the Lasso model with log transformation offers model simplicity and generalizability. Both models are valuable for understanding salary determinants across different experience levels, job categories, and work settings. However, the log-transformation enhances both the statistical performance of the model and the interpretability of the results, making it our recommended approach in modeling salary data.

# Report Summary

The analysis focused on identifying key drivers of salary within data-related professions, leveraging multiple regression techniques and regularization methods to develop predictive models. The report journey included rigorous data preprocessing, exploratory data analysis (EDA), and systematic model tuning, resulting in the selection of a final model based on Lasso regression.

### Analysis Insights

  - Key Predictors: The most impactful predictors of salary were identified as experience level, employment type, company size, job category, and work setting. Specifically, executive experience, technical job categories such as Machine Learning and Data Science, and full-time employment were associated with higher salaries, while freelance and operational roles like Data Quality and Data Analysis were linked to lower compensation.
  
  - Model Selection: Various models were evaluated, including Stepwise Regression, Lasso, and Ridge Regression. The Lasso model was selected as the final model due to its superior performance in balancing model fit and complexity, reflected by the lowest AIC and BIC values and its effective feature selection capability. While the Stepwise model explained a marginally higher proportion of salary variance, it was more prone to overfitting, as suggested by its higher AIC and BIC scores.
  
  - Model Diagnostics: During model refinement, interaction terms between experience_level, employment_type, and job_category were explored to assess potential non-linear relationships and synergies between these variables. For example, higher experience levels combined with specific employment types, such as freelance or part-time, could influence salary differently than when considered individually. However, upon testing, these interaction terms did not significantly improve the model's predictive power and added unnecessary complexity. Thus, interaction terms were ultimately excluded from the final model to maintain simplicity and interpretability without sacrificing accuracy.
  
  - Interpretability and Practical Relevance: The final model provides practical insights into the salary structure within data jobs, highlighting factors that professionals and companies might prioritize. For instance, the significant salary premiums associated with remote work settings reflect industry trends favoring flexible work arrangements.

# Further Considerations

While the report offers a thorough examination of salary predictors within data-related professions, several additional aspects could be explored in future analyses.

### Recomendations

  - Inclusion of Additional Predictors: Incorporating external economic indicators, geographic location specifics, and skill certifications could further improve the model’s explanatory power. These variables often influence salary but were beyond this report's scope due to dataset limitations.

  - Temporal Analysis: Salaries in data-related fields can fluctuate due to industry trends and economic cycles. A temporal analysis considering how these relationships evolve over time, using time-series techniques or cohort-based segmentation, could provide more dynamic insights.

  - Interaction Effects: Interaction terms were explored to assess non-linear relationships between experience_level, employment_type, and job_category. However, they were ultimately excluded as they did not significantly enhance the predictive power of the model and added complexity without substantial benefits.

  - Handling Outliers: To improve model stability, we identified and removed outliers using the IQR method, focusing on extreme salary values. Outliers, especially at the higher end of the salary range, can disproportionately influence regression models, often inflating coefficients and leading to skewed results. For instance, exceptionally high salaries may reflect rare or specialized roles that are not representative of the general data trends. By excluding these outliers, we achieved a more stable model that better represents the majority of cases in the dataset. This approach enhances model robustness, ensuring that predictions are less susceptible to extreme variations and thus more generalizable.

  - Cross-Validation Methods: While cross-validation was employed to tune Lasso and Ridge models, exploring additional validation methods, such as k-fold cross-validation or bootstrapping, could provide further robustness to model selection.
  
  - Possibilities for further analysis: This analysis provides valuable insights into factors influencing salary within the data industry. However, further work could expand the analysis by incorporating additional predictors, such as industry-specific factors or geographical location, which may also impact salary. Additionally, non-linear models, such as decision trees or random forests, could be explored to capture more complex relationships that may not be fully addressed by linear regression. These future enhancements would provide a more comprehensive view of salary determinants and further improve predictive accuracy.

# End.